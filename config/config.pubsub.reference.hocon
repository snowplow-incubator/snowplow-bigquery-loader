{
  # -- Full license text available in LICENSE.md
  "license" {
    "accept": true
  }

  "input": {
    # -- pubsub subscription for the source of enriched events
    "subscription": "projects/myproject/subscriptions/snowplow-enriched"

    # -- How many threads are used by the pubsub client library for fetching events
    "parallelPullCount": 3

    # -- How many bytes can be buffered by the loader app before blocking the pubsub client library
    # -- from fetching more events.
    # -- This is a balance between memory usage vs how efficiently the app can operate.  The default value works well.
    "bufferMaxBytes": 1000000

    # -- For how long the pubsub client library will continue to re-extend the ack deadline of an unprocessed event.
    "maxAckExtensionPeriod": "1 hour"

    # -- Sets min/max boundaries on the value by which an ack deadline is extended.
    # -- The actual value used is guided by runtime statistics collected by the pubsub client library.
    "minDurationPerAckExtension": "60 seconds"
    "maxDurationPerAckExtension": "600 seconds"
  }

  "output": {

    "good": {
      # -- the GCP project to which the BigQuery dataset belongs.
      "project": "my-project"

      # -- the BigQuery dataset to which events will be loaded
      "dataset": "my-project"

      # -- the table within the BigQuery dataset to which events will be loaded
      "table": "events"

      # -- optional, service account credentials (JSON). If not set, default credentials will be
      # -- sourced from the usual locations.
      "credentials": ${?SERVICE_ACCOUNT_CREDENTIALS}
    }

    "bad": {
      # -- output pubsub topic for emitting failed events that could not be processed
      "topic": "projects/myproject/topics/snowplow-bad"

      # -- Failed sends events to pubsub in batches not exceeding this size.
      "batchSize": 100
      # -- Failed events to pubsub in batches not exceeding this size number of bytes
      "requestByteThreshold": 1000000
    }

  }

  "batching": {

    # -- Events are emitted to BigQuery when the batch reaches this size in bytes
    "maxBytes": 16000000

    # -- Events are emitted to BigQuery for a maximum of this duration, even if the `maxBytes` size has not been reached
    "maxDelay": "1 second"

    # -- How many batches can we send simultaneously over the network to BigQuery.
    "writeBatchConcurrency":  1
  }

  "retries": {
    # -- Base delay for exponential backoff on retriable runtime errors, e.g. server errors and network errors.
    "backoff": "1 second"

    # -- Max number of attempts on retriable runtime errors. After exceeding this number, the loader will crash and exit.
    "attempts": 5

    # -- Base delay for exponential backoff on errors that can only be fixed by manual intervention.
    # -- Examples include authentication errors and permissions errors.
    # -- This class of errors are reported periodically to the monitoring webhook.
    "fixableBackoff": "30 seconds"
  }

  "monitoring": {
    "metrics": {

      # -- Send runtime metrics to a statsd server
      "statsd": {
        "hostname": "127.0.0.1"
        "port": 8125

        # -- Map of key/value pairs to be send along with the metric
        "tags": {
          "myTag": "xyz"
        }

        # -- How often to report metrics
        "period": "1 minute"

        # -- Prefix used for the metric name when sending to statsd
        "prefix": "snowplow.bigquery-loader"
      }
    }

    # -- Report unexpected runtime exceptions to Sentry
    "sentry": {
      "dsn": "https://public@sentry.example.com/1"

      # -- Map of key/value pairs to be included as tags
      "tags": {
        "myTag": "xyz"
      }
    }
  }

  # -- Optional, configure telemetry
  # -- All the fields are optional
  "telemetry": {

    # -- Set to true to disable telemetry
    "disable": false

    # -- Interval for the heartbeat event
    "interval": 15 minutes

    # -- HTTP method used to send the heartbeat event
    "method": POST

    # -- URI of the collector receiving the heartbeat event
    "collectorUri": collector-g.snowplowanalytics.com

    # -- Port of the collector receiving the heartbeat event
    "collectorPort": 443

    # -- Whether to use https or not
    "secure": true

    # -- Identifier intended to tie events together across modules,
    # -- infrastructure and apps when used consistently
    "userProvidedId": my_pipeline

    # -- ID automatically generated upon running a modules deployment script
    # -- Intended to identify each independent module, and the infrastructure it controls
    "autoGeneratedId": hfy67e5ydhtrd

    # -- Unique identifier for the VM instance
    # -- Unique for each instance of the app running within a module
    "instanceId": 665bhft5u6udjf

    # -- Name of the terraform module that deployed the app
    "moduleName": bigquery-loader-vmss

    # -- Version of the terraform module that deployed the app
    "moduleVersion": 1.0.0
  }
}
