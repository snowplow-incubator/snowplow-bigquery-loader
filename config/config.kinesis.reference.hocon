{
  # -- Full license text available in LICENSE.md
  "license" {
    "accept": true
  }

  "input": {
    # -- kinesis stream for the source of enriched events
    "streamName": "snowplow-enriched-events"

    # -- name to use for the KCL dynamodb table
    "appName": "snowplow-bigquery-loader"

    # -- From where the app should start consuming if this is the first time it is run.
    # -- On subsequent runs, it will always resume from where it last checkpointed.
    "initialPosition": {
      # -- Options are `TRIM_HORIZON` for the oldest available events, `LATEST` for latest events,
      # -- or `AT_TIMESTAMP` to start consuming from events written at a particular time.
      "type": "TRIM_HORIZON"

      # -- Only required if `initialPosition.type` is AT_TIMESTAMP
      "timestamp": "2020-07-17T10:00:00Z" # Required for AT_TIMESTAMP
    }

    # -- How the underlying Kinesis client should fetch events from the stream
    "retrievalMode": {
      # -- Options are "Polling" for the client to poll Kinesis for more events when needed
      # -- or "FanOut" to enabled Kinesis's Enhanced Fan Out feature using HTTP/2
      "type": "Polling"

      # -- Only used if retrieval mode is type Polling. How many events the client may fetch in a single poll.
      "maxRecords": 1000
    }

    # -- Name of this KCL worker used in the dynamodb lease table
    "workerIdentifier": ${HOSTNAME}

    # -- Duration of shard leases. KCL workers must periodically refresh leases in the dynamodb table before this duration expires.
    "leaseDuration": "10 seconds"

    # -- Controls how to pick the max number of leases to steal at one time.
    # -- E.g. If there are 4 available processors, and maxLeasesToStealAtOneTimeFactor = 2.0, then allow the KCL to steal up to 8 leases.
    # -- Allows bigger instances to more quickly acquire the shard-leases they need to combat latency
    "maxLeasesToStealAtOneTimeFactor": 2.0

    # -- Configures how to backoff and retry in case of DynamoDB provisioned throughput limits
    "checkpointThrottledBackoffPolicy": {
      "minBackoff": "100 millis"
      "maxBackoff": "1 second"
    }
  }

  "output": {

    "good": {
      # -- the GCP project to which the BigQuery dataset belongs.
      "project": "my-project"

      # -- the BigQuery dataset to which events will be loaded
      "dataset": "my-dataset"

      # -- the table within the BigQuery dataset to which events will be loaded
      "table": "events"

      # -- optional, service account credentials (JSON). If not set, default credentials will be
      # -- sourced from the usual locations.
      "credentials": ${?SERVICE_ACCOUNT_CREDENTIALS}
    }

    "bad": {
      # -- output kinesis stream for emitting failed events that could not be processed
      "streamName": "bad"

      # -- how to retry sending failed events if we exceed the kinesis write throughput limits
      "throttledBackoffPolicy": {
        "minBackoff": "100 milliseconds"
        "maxBackoff": "1 second"
      }

      # -- the maximum allowed to records we are allowed to send to Kinesis in 1 PutRecords request
      "recordLimit": 500

      # -- the maximum allowed to bytes we are allowed to send to Kinesis in 1 PutRecords request
      "byteLimit": 5242880
    }

  }

  "batching": {

    # -- Events are emitted to BigQuery when the batch reaches this size in bytes
    "maxBytes": 16000000

    # -- Events are emitted to BigQuery for a maximum of this duration, even if the `maxBytes` size has not been reached
    "maxDelay": "1 second"

    # -- How many batches can we send simultaneously over the network to BigQuery.
    "writeBatchConcurrency":  2
  }

  "cpuParallelism" {

    # - Controls how many batches of bytes we can parse into enriched events simultaneously.
    # -- E.g. If there are 2 cores, and parseBytesFactor = 0.1, then only one batch gets processed at a time.
    # -- Adjusting this value can cause the app to use more or less of the available CPU.
    "parseBytesFactor": 0.1

    # - Controls how many batches of enriched events we can transform into BigQuery format simultaneously.
    # -- E.g. If there are 4 cores, and parseBytesFactor = 0.75, then 3 batches gets processed in parallel.
    # -- Adjusting this value can cause the app to use more or less of the available CPU.
    "transformFactor": 0.75
  }

  # Retry configuration for BigQuery operation failures
  "retries": {

    # -- Configures exponential backoff on errors related to how BigQuery is set up for this loader.
    # -- Examples include authentication errors and permissions errors.
    # -- This class of errors are reported periodically to the monitoring webhook.
    "setupErrors": {
      "delay": "30 seconds"
    }

    # -- Configures exponential backoff errors that are likely to be transient.
    # -- Examples include server errors and network errors
    "transientErrors": {
      "delay": "1 second"
      "attempts": 5
    }

    # -- Configures backoff when waiting for the BigQuery Writer to detect that we have altered the
    # -- table by adding new columns
    "alterTableWait": {
      "delay": "1 second"
    }

    # -- Relevant when the BigQuery table is close to exceeding the limit on max allowed columns in a single table.
    # -- The loader will ignore a failure to alter the table due to too many columns, and it will continue to run.
    # -- Some events will inevitably go to the failed events output topic until new columns have been added.
    # -- This param configures how often the loader will retry to alter the table after an earlier failure.
    "tooManyColumns": {
      "delay": "300 seconds"
    }
  }

  # -- Schemas that won't be loaded to BigQuery. Optional, default value []
  "skipSchemas": [
    "iglu:com.acme/skipped1/jsonschema/1-0-0",
    "iglu:com.acme/skipped2/jsonschema/1-0-*",
    "iglu:com.acme/skipped3/jsonschema/1-*-*",
    "iglu:com.acme/skipped4/jsonschema/*-*-*"
  ]

  # -- Schemas for which to use the legacy column style used by the v1 BigQuery loader
  # -- For these columns, there is a column per _minor_ version of each schema.
  "legacyColumns": [
    "iglu:com.acme/legacy/jsonschema/1-*-*"
    "iglu:com.acme/legacy/jsonschema/2-*-*"
  ]

  # -- Whether the loader should load to legacy columns irrespective of `legacyColumns` configuration.
  # -- Change to `true` so events go to the legacy columns.
  "legacyColumnMode": false

  # -- Whether the loader should crash and exit if it fails to resolve an Iglu Schema.
  # -- We recommend `true` because Snowplow enriched events have already passed validation, so a missing schema normally
  # -- indicates an error that needs addressing.
  # -- Change to `false` so events go the failed events stream instead of crashing the loader.
  "exitOnMissingIgluSchema": true

  # -- Configuration of internal http client used for iglu resolver, alerts and telemetry
  "http": {
    "client": {
      "maxConnectionsPerServer": 4
    }
  }

  "monitoring": {
    "metrics": {

      # -- Send runtime metrics to a statsd server
      "statsd": {
        "hostname": "127.0.0.1"
        "port": 8125

        # -- Map of key/value pairs to be send along with the metric
        "tags": {
          "myTag": "xyz"
        }

        # -- How often to report metrics
        "period": "1 minute"

        # -- Prefix used for the metric name when sending to statsd
        "prefix": "snowplow.bigquery.loader"
      }
    }

    # -- Report unexpected runtime exceptions to Sentry
    "sentry": {
      "dsn": "https://public@sentry.example.com/1"

      # -- Map of key/value pairs to be included as tags
      "tags": {
        "myTag": "xyz"
      }
    }

    # -- Report alerts and heartbeats to the webhook
    "webhook": {
      # An actual HTTP endpoint
      "endpoint": "https://webhook.acme.com",
      # Set of arbitrary key-value pairs attached to the payload
      "tags": {
        "pipeline": "production"
      }
      # How often to send the heartbeat event
      "heartbeat": "60.minutes"
    }
  }

  # -- Optional, configure telemetry
  # -- All the fields are optional
  "telemetry": {

    # -- Set to true to disable telemetry
    "disable": false

    # -- Interval for the heartbeat event
    "interval": 15 minutes

    # -- HTTP method used to send the heartbeat event
    "method": POST

    # -- URI of the collector receiving the heartbeat event
    "collectorUri": collector-g.snowplowanalytics.com

    # -- Port of the collector receiving the heartbeat event
    "collectorPort": 443

    # -- Whether to use https or not
    "secure": true

    # -- Identifier intended to tie events together across modules,
    # -- infrastructure and apps when used consistently
    "userProvidedId": my_pipeline

    # -- ID automatically generated upon running a modules deployment script
    # -- Intended to identify each independent module, and the infrastructure it controls
    "autoGeneratedId": hfy67e5ydhtrd

    # -- Unique identifier for the VM instance
    # -- Unique for each instance of the app running within a module
    "instanceId": 665bhft5u6udjf

    # -- Name of the terraform module that deployed the app
    "moduleName": bigquery-loader-vmss

    # -- Version of the terraform module that deployed the app
    "moduleVersion": 1.0.0
  }
}
