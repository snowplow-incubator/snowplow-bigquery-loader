{
  # The GCP project in which all required Pub/Sub, BigQuery and GCS resources are hosted
  "projectId": "com-acme"

  "loader": {
    # Enriched events subscription consumed by Loader and Streamloader
    "input": {
      "type": "PubSub"
      "subscription": "enriched-sub"
    }

    "output": {
      # Events that pass validation are written to BigQuery
      "good": {
        "type": "BigQuery"
        "datasetId": "snowplow"
        "tableId": "events"
      }

      # Events that fail validation are written to Pub/Sub
      "bad": {
        "type": "PubSub"
        "topic": "bad-topic"
      }

      # Observed types are regularly written to Pub/Sub
      # Consumed by Mutator
      "types": {
        "type": "PubSub"
        "topic": "types-topic"
      }

      # Validated events that cannot be loaded into BigQuery are written to Pub/Sub
      # Consumed by Repeater
      "failedInserts": {
        "type": "PubSub"
        "topic": "failed-inserts-topic"
      }
    }

    # Specifies the loading mode
    "loadMode": {
      # Streaming Inserts API, recommended
      "type": "StreamingInserts"
      "retry": false

      # Load jobs API, experimental
      # "type": "FileLoads"
      # "frequency": 60000
    }

    # Advanced settings for the loader.input subscriber
    # For defaults refer to https://github.com/permutive/fs2-google-pubsub/blob/master/fs2-google-pubsub-grpc/src/main/scala/com/permutive/pubsub/consumer/grpc/PubsubGoogleConsumerConfig.scala
    "consumerSettings": {
      "maxQueueSize": 1000
      "parallelPullCount": 3
      "maxAckExtensionPeriod": "10 seconds"
      "awaitTerminatePeriod": "30 seconds"
    }

    # Advanced settings for the loader.output sinks
    "sinkSettings": {
      "good": {
        # For recommended number of records in each request, see https://cloud.google.com/bigquery/quotas#streaming_inserts
        "bqWriteRequestThreshold": 500
        "bqWriteRequestTimeout": "1 second"
        # For the HTTP request size limit, see https://cloud.google.com/bigquery/quotas#streaminginserts
        "bqWriteRequestSizeLimit": 10000000
        "sinkConcurrency": 64
      }

      "bad": {
        "producerBatchSize": 8
        "producerDelayThreshold": "2 seconds"
        "sinkConcurrency": 64
      }

      "types": {
        "batchThreshold": 10
        "batchTimeout": "30 seconds"
        "producerBatchSize": 4
        "producerDelayThreshold": "200 ms"
        "sinkConcurrency": 64
      }

      "failedInserts": {
        "producerBatchSize": 8
        "producerDelayThreshold": "2 seconds"
      }
    }

    # For recommended initial and max delay values, see https://cloud.google.com/bigquery/sla (Definition of "Back-off Requirements")
    "retrySettings": {
        "type": "BigQueryRetrySettings"
        "initialDelay": 1 # secs
        "delayMultiplier": 1.5
        "maxDelay": 32 # secs
        "totalTimeout": 5 # 5 mins gives us 30 attempts with 1 sec initial delay and 1.5 multiplier
    }

    # Maximum time to wait for graceful shutdown to complete
    "terminationTimeout": "1 minute"
  }

  "mutator": {
    # Observed types subscription consumed by Mutator
    # Must be attached to the loader.output.types.topic
    "input": {
      "type": "PubSub"
      "subscription": "types-sub"
    }

    "output": {
      # Mutator will update the table to which Loader / Streamloader writes
      "good": ${loader.output.good}
    }
  }

  "repeater": {
    "input": {
      # Failed inserts subscription consumed by Repeater
      # Must be attached to the loader.output.failedInserts.topic
      "type": "PubSub"
      "subscription": "failed-inserts-sub"
    }

    "output": {
      # Repeater writes events to the same table as Loader / Streamloader
      "good": ${loader.output.good}

      # Failed inserts that repeatedly fail to be inserted into BigQuery are stored on GCS
      "deadLetters": {
        "type": "Gcs"
        "bucket": "gs://dead-letter-bucket"
      }
    }
  }

  # Optional
  "monitoring": {
    # For StreamLoader deployments
    "statsd": {
      "hostname": "statsd.acme.gl"
      "port": 1024
      "tags": {
        # You can use environment variables
        # "worker": ${HOST}
      }
      # Reporting period with time unit
      "period": "10 min"
      # Optional
      "prefix": "snowplow.monitoring"
    }
    "stdout": {
      # Reporting period with time unit
      "period": "10 min"
      # Optional
      "prefix": "snowplow.monitoring"
    }
    # For tracking runtime exceptions
    "sentry": {
      "dsn": "http://sentry.acme.com"
    }

    # For Loader deployments
    # "dropwizard": {
    #   # Reporting period with time unit
    #   "period": "1000 ms"
    # }
    }
}
